{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WWC_ML_workshop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RhythmOfRiora/Projects/blob/master/Copy_of_WWC_ML_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFjdEJdIly_z",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**\n",
        "\n",
        "Python uses imported libraries to perform a wide range of functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsetCPgalwYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt #Matplotlib is used to create visualisations of the data\n",
        "from matplotlib.colors import ListedColormap\n",
        "from mpl_toolkits.mplot3d import Axes3D#Axes is used to create figures\n",
        "import numpy as np#Numpy is used to support multi-dimensional arrays/matrices and perform mathematical functions on them\n",
        "\n",
        "#Sklearn is a machine learning library that contains a large number of ml \n",
        "# algorithms and ways to analyse them\n",
        "\n",
        "from sklearn import linear_model,  svm, datasets\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.utils.multiclass import unique_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Ni-9421fu2",
        "colab_type": "text"
      },
      "source": [
        "**Load in the dataset**\n",
        "\n",
        "SKLearn has a selection of datasets we can use. This includes the iris dataset, a great example of classification. Consists of 150 samples, of 3 types of iris's. We have to be able to identify the 3 different types of irises using petals sepal length and width. These flowers look similar to the naked eye, similar in colour and shape but have some key differences that will make it simple for our classification models to differienate them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky6JWyaeGLh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris=datasets.load_iris()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbs0fTA0bDJl",
        "colab_type": "text"
      },
      "source": [
        "**Class names**\n",
        "\n",
        "There are 3 different types of iris's, and it will be our class names/labels for the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JV0d2Qebulv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  iris.target_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myLrrBfr2NNn",
        "colab_type": "text"
      },
      "source": [
        "**Feature names**\n",
        "\n",
        "The dataset we are using is made of 4 features/input variable types, these store all the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJYpoN28cEOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " iris.feature_names\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tInOxbQ_2fNi",
        "colab_type": "text"
      },
      "source": [
        "**Labelled sample**\n",
        "\n",
        "An particular instance of the dataset, containing both the label and the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q51nkYxKqkS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example=[iris.data[0], iris.target[0]]\n",
        "print(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-b54Xrg3oEc",
        "colab_type": "text"
      },
      "source": [
        "**Visualize the data**\n",
        "\n",
        "It's important to be able to understand the data we're using, and the best way to do this is by visualising it. What data is being visualised below in the two plots? Note in the first graph there is a large amount of overlap between the 2nd and 3rd classes but with the 2nd graph there is a less of an overlap but some still remains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBg-WrMihr0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris.data[:,:2 ]  # Take 1st and 2nd feature,by taking all the data from the first two arrays in the multi-dimensional array\n",
        "y = iris.target\n",
        "\n",
        "# Return the min and max by returning both,using a comma to separate the parameters\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "\n",
        "plt.figure(2, figsize=(8, 6))\n",
        "plt.clf()\n",
        "\n",
        "# Plot the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,\n",
        "            edgecolor='k')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu0-gv5XiSIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris.data[:,2: ]  # Take 3rd and 4th feature,by taking all the data from the first two arrays in the multi-dimensional array\n",
        "y = iris.target\n",
        "\n",
        "# Return the min and max by returning both,using a comma to separate the parameters\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "\n",
        "plt.figure(2, figsize=(8, 6))\n",
        "plt.clf()\n",
        "\n",
        "# Plot the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,\n",
        "            edgecolor='k')\n",
        "plt.xlabel('X3')\n",
        "plt.ylabel('X4')\n",
        "\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJkQLsyu4ec6",
        "colab_type": "text"
      },
      "source": [
        "**Visualise all dimensions**\n",
        "\n",
        "The previous visulisations only look at the first two dimensions, but there are 4 dimensions to the dataset. PCA is a way of visualising the data by squashing all the information into a smaller number of dimensions. These are represented via eigenvectors, where we can see there is very little overlap between the classes. This shows that our classifier should work well on this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSHh5HcpjLGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(1, figsize=(8, 6))\n",
        "ax = Axes3D(fig, elev=-150, azim=110)\n",
        "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
        "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,\n",
        "           cmap=plt.cm.Set1, edgecolor='k', s=40)\n",
        "ax.set_title(\"First three PCA directions\")\n",
        "ax.set_xlabel(\"1st eigenvector\")\n",
        "ax.w_xaxis.set_ticklabels([])\n",
        "ax.set_ylabel(\"2nd eigenvector\")\n",
        "ax.w_yaxis.set_ticklabels([])\n",
        "ax.set_zlabel(\"3rd eigenvector\")\n",
        "ax.w_zaxis.set_ticklabels([])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0r1K8IT7Q88",
        "colab_type": "text"
      },
      "source": [
        "**Importance of splitting the data into train and test split**\n",
        "\n",
        "When training a dataset we want to give it as much data as possible, so would it make sense to give the dataset all of our data? Why? Why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmD_kOaOjqcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=45)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcpxQfBr54Hw",
        "colab_type": "text"
      },
      "source": [
        "**KNN Neighbours classifier**\n",
        "\n",
        "KNN neighbours classifier finds its labels by copying the majority of the labels its neighbours  with. The label is affected by how many examples are considered neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5uAPahQkZvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply Knn classifier\n",
        "#Test different sizes of n neighbours 1,3,5,15\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_classifier.fit(X_train, y_train) \n",
        "\n",
        "\n",
        "# predict the response\n",
        "knn_prediction = knn_classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6m3mnX-6qI3",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy**\n",
        "\n",
        "The percentage of times the classifer selects the correct label in the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd1Rw9rd6qn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate accuracy\n",
        "print(accuracy_score(y_test, knn_prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpmrwm4M7pVw",
        "colab_type": "text"
      },
      "source": [
        "**KNN neighbours visulisation**\n",
        "\n",
        "Changing the number of neighbours affects the accuracy, which classes are selected. Test to find the best accuracy for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YPjY-2OjEnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test different sizes of n neighbours 1,3,5,10,15 -using only sepal length and width\n",
        "n_neighbors = 15\n",
        "\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "# Create color maps\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
        "\n",
        "for items in [[X_train[:,:2],y_train, \"Train\"],[X_test[:,:2],y_test, \"Test\"]]:\n",
        "    \n",
        "    X=items[0]\n",
        "    y=items[1]\n",
        "   # we create an instance of Neighbours Classifier and fit the data.\n",
        "    clf =KNeighborsClassifier(n_neighbors, weights=\"uniform\")\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    #Two parameters to find the minimum and maximum, comma to indicate the two parts are doing     \n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
        "                edgecolor='k', s=20)\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.title(\"3-Class classification \"+items[2]+\" k =\"+str(n_neighbors))\n",
        "    \n",
        "knn_visualisation_classifier = KNeighborsClassifier(n_neighbors)\n",
        "knn_visualisation_classifier.fit(X_train[:,:2], y_train) \n",
        "\n",
        "\n",
        "# predict the response\n",
        "knn_visualisation_prediction = knn_visualisation_classifier.predict(X_test[:,:2])\n",
        "\n",
        "# evaluate accuracy\n",
        "print(accuracy_score(y_test, knn_visualisation_prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiL8Xhs99zA6",
        "colab_type": "text"
      },
      "source": [
        "**Logistic regression**\n",
        "\n",
        "Logistic regression is a classification model that find a line of best fit in which on either sides are different classes. This can be extended to work on multiclasses too, by finding lines that will work in comparing each class separately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN8pyytNkhCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic regression\n",
        "\n",
        "# Create logistic regression object\n",
        "# Try different solvers:  'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
        "logistic_regression_classifier = linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
        "\n",
        "# Train the model using the training sets\n",
        "logistic_regression_classifier.fit(X_train,y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "logistic_regression_classifier_prediction = logistic_regression_classifier.predict(X_test)\n",
        "\n",
        "\n",
        "# evaluate accuracy\n",
        "print(accuracy_score(y_test, logistic_regression_classifier_prediction))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv2hbTpvFKWM",
        "colab_type": "text"
      },
      "source": [
        "**Feel free to try other classifiers!**\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21DFJe8lGOrg",
        "colab_type": "text"
      },
      "source": [
        "**Precision and Recall**\n",
        "\n",
        "Accuracy shouldn't be the only considered\n",
        "\n",
        "*Precision-* what proportion of positive identifications were actually correct. \n",
        "\n",
        "*Recall-* what proportion of actual positives were identified correctly\n",
        "\n",
        "Useful link: https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk0dKglXkuEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explain how accuracy isn't everything\n",
        "# Recall, precision. \n",
        "knn_precision, knn_recall, knn_fbeta_score, knn_support=precision_recall_fscore_support(y_test, knn_prediction, average='macro')\n",
        "print(knn_precision)\n",
        "print(knn_recall)\n",
        "\n",
        "lr_precision, lr_recall, lr_fbeta_score, lr_support=precision_recall_fscore_support(y_test, logistic_regression_classifier_prediction, average='macro')\n",
        "print(lr_precision)\n",
        "print(lr_recall)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfvesuT9lGE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Work on next tabular dataset. \n",
        "breast_cancer=datasets.load_breast_cancer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOw_4AAW6yrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}